{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Jirkovo místečko\n",
    "* [ ] task definition (suitable df,  df preparation, curse of dimensionality, dimensionality reduction, df preparation)\n",
    "* [ ] Binary vs Multiple\n",
    "* [ ] Binary classification (ROC, Precision, Accuracy,Sensitivity, Specificity)\n",
    "* [ ] GLM + SVM\n",
    "* [ ] Grid Search\n",
    "* [ ] Validation, Overtraining"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks\n",
    "* [ ] https://karpathy.github.io/neuralnets/\n",
    "* [ ] https://playground.tensorflow.org/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XOR using NN\n",
    "Let's predict Exclusive OR using NN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "+---+---+---------+\n",
    "| A | B | A XOR B |\n",
    "+---+---+---------+\n",
    "| 1 | 1 | 0       |\n",
    "+---+---+---------+\n",
    "| 1 | 0 | 1       |\n",
    "+---+---+---------+\n",
    "| 0 | 1 | 1       |\n",
    "+---+---+---------+\n",
    "| 0 | 0 | 0       |\n",
    "+---+---+---------+\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "It's a binary classification problem, and a supervised one. Our task is to create a neural network that will predict the values of one logical function given the inputs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's assume the simple shallow architecture with hidden layer\n",
    "![NNet](pics/NNEt.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* `X1,X2` = data input\n",
    "* `N1,N2,N3` = neurons\n",
    "* `B1,B2` = bias\n",
    "* `W..` = weights\n",
    "* `b..` = bias weights\n",
    "* `Y` = output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bias node is \"always on\" -- in NN has the role of the intercept and it's providing a way to get non-zero output on zero inputs. Without bias, the activation of features = 0 would be always zero. When using sigmoid function for example, the output of the (0,0) would be 0.5. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll assume sigmoid neurons:\n",
    "* they accept real values as inputs\n",
    "* the value of activation is a dot product of weights and inputs (+ bias), i.e. `W*out_{prev} + b`\n",
    "* output is a sigmoid function of its activation value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1/(1+np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid_derivative(x):\n",
    "    \"\"\"\n",
    "    x is assumed to be sigmoid function!\n",
    "    \"\"\"\n",
    "    return x*(1-x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.arange(-10,10,0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(x, sigmoid(x), x, sigmoid_derivative(sigmoid(x)))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Learning\n",
    "Information is stored in connections between the neurons -- the weights. NN learns by updating its weights according to a learning algorithm that helps it converge to the expected output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Initialize the weights and biases randomly.\n",
    "* Iterate over the data\n",
    "    * Compute the predicted output\n",
    "    * Compute the loss (distance from the data)\n",
    "    * `W(new) = W(old) — α ∆W`\n",
    "    * `b(new) = b(old) — α ∆b`\n",
    "* Repeat until the error is minimal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Update step is assumed in the direction of gradient descent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two parts to the learning algorithm\n",
    "* Forward pass (computation of the predicted output)\n",
    "* Backward pass, aka backpropagation (change of the weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data\n",
    "Let's create our learning dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([[0,0],[0,1],[1,0],[1,1]])\n",
    "Y = np.array([[0],[1],[1],[0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initialization of the weights and biases\n",
    "Let's sample normal distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_params(n_X,n_H,n_Y):\n",
    "    \"\"\"\n",
    "    n_X ... number of input layer neurons\n",
    "    n_H ... number of hidden layer neurons\n",
    "    n_Y ... number of output layer neurons\n",
    "    \"\"\"\n",
    "    W1 = np.random.randn(n_X, n_H)\n",
    "    b1 = np.zeros((1, n_H))\n",
    "    W2 = np.random.randn(n_H, n_Y)\n",
    "    b2 = np.zeros((1, n_Y))\n",
    "    \n",
    "    return W1,b1,W2,b2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_X, n_H, n_Y = 2,2,1\n",
    "W1,b1,W2,b2 = init_params(2,2,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial hidden weights: [-0.81121937  0.72923401] [ 0.43503362 -0.15866546]\n",
      "Initial hidden biases: [0. 0.]\n",
      "Initial output weights: [-0.53002286] [0.09047724]\n",
      "Initial output biases: [0.]\n"
     ]
    }
   ],
   "source": [
    "print(\"Initial hidden weights: \",end='')\n",
    "print(*W1)\n",
    "print(\"Initial hidden biases: \",end='')\n",
    "print(*b1)\n",
    "print(\"Initial output weights: \",end='')\n",
    "print(*W2)\n",
    "print(\"Initial output biases: \",end='')\n",
    "print(*b2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10000\n",
    "lr = 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Forward propagation\n",
    "Computing the predicted values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_prop(X,W1,b1,W2,b2):\n",
    "    hidden_layer_activation = np.dot(X,W1) + b1\n",
    "    hidden_layer_output = sigmoid(hidden_layer_activation)\n",
    "\n",
    "    output_layer_activation = np.dot(hidden_layer_output,W2) + b2 \n",
    "    Y_hat = sigmoid(output_layer_activation)\n",
    "    \n",
    "    return hidden_layer_output, Y_hat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Error (loss) function\n",
    "Let's use mean squared error. Typically used rather for regression problems, but we'll ignore that for now.\n",
    "\n",
    "$E = \\frac{1}{2}(Y - Y_{hat})^2$\n",
    "\n",
    "We're going to need it's derivative (w.r.t. $Y_{hat}$)\n",
    "\n",
    "$\\frac{\\partial E}{\\partial Y_{hat}} = -(Y - Y_{hat})$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function(Y,Y_hat):\n",
    "    loss = 1/2*np.sum(np.power(Y-Y_hat,2))\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_derivative(Y,Y_hat):\n",
    "    d_loss = -(Y - Y_hat)\n",
    "    \n",
    "    return d_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Backpropagation\n",
    "Goal of the backpropagation is to change the weights in order to minimize the error/loss. Since the outcome is a function of activation and further activation is a function of weights, so we want to know\n",
    "\n",
    "$\\frac{\\partial E}{\\partial w21} = \\frac{\\partial E}{\\partial Y_{hat}} * \\frac{\\partial Y_{hat}}{\\partial net_{N3}} * \\frac{\\partial net_{N3}}{\\partial w21}$\n",
    "\n",
    "\n",
    "We already know the derivative of the error with respect to the prediction. The second term is derivative of the sigmoid\n",
    "\n",
    "$\\frac{\\partial Y_{hat}}{\\partial net_{N3}} = Y_{hat} * (1 - Y_{hat})$\n",
    "\n",
    "and the last term is a derivative of the activation by weights, so output of the hidden layer. \n",
    "\n",
    "$\\frac{\\partial net_{N3}}{\\partial w21} = out_{N1}$\n",
    "\n",
    "The same logic applied to other weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def back_prop(d_loss, Y_hat, W2, hidden_layer_output):\n",
    "    d_Y_hat = d_loss * sigmoid_derivative(Y_hat)\n",
    "    error_hidden_layer = d_Y_hat.dot(W2.T)\n",
    "    d_hidden_layer = error_hidden_layer * sigmoid_derivative(hidden_layer_output)\n",
    "    \n",
    "    return d_Y_hat, d_hidden_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after iteration# 0: 0.505482\n",
      "Loss after iteration# 1000: 0.500076\n",
      "Loss after iteration# 2000: 0.499695\n",
      "Loss after iteration# 3000: 0.499029\n",
      "Loss after iteration# 4000: 0.496316\n",
      "Loss after iteration# 5000: 0.477242\n",
      "Loss after iteration# 6000: 0.404063\n",
      "Loss after iteration# 7000: 0.254559\n",
      "Loss after iteration# 8000: 0.081355\n",
      "Loss after iteration# 9000: 0.038809\n"
     ]
    }
   ],
   "source": [
    "#Training algorithm\n",
    "for i in range(epochs):\n",
    "    #Forward Propagation\n",
    "    hidden_layer_output, Y_hat = forward_prop(X,W1,b1,W2,b2)\n",
    "\n",
    "    # Compute loss and derivatives\n",
    "    loss = loss_function(Y,Y_hat)\n",
    "    d_loss = loss_derivative(Y,Y_hat)\n",
    "    \n",
    "    #Backpropagation\n",
    "    d_Y_hat, d_hidden_layer = back_prop(d_loss, Y_hat, W2, hidden_layer_output)\n",
    "    \n",
    "    #Updating Weights and Biases (Gradient descent)\n",
    "    W2 = W2 - hidden_layer_output.T.dot(d_Y_hat) * lr\n",
    "    b2 = b2 - np.sum(d_Y_hat,axis=0,keepdims=True) * lr\n",
    "    W1 = W1 - X.T.dot(d_hidden_layer) * lr\n",
    "    b1 = b1 - np.sum(d_hidden_layer,axis=0,keepdims=True) * lr\n",
    "    \n",
    "    if(i%1000 == 0):\n",
    "        print('Loss after iteration# {:d}: {:f}'.format(i, loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final hidden weights: [-5.64861542  4.83533818] [ 5.70655464 -4.54853647]\n",
      "Final hidden bias: [2.97110367 2.28803863]\n",
      "Final output weights: [-5.24747215] [-5.36563094]\n",
      "Final output bias: [7.74025937]\n",
      "\n",
      "Output from neural network after 10,000 epochs: [0.10692546] [0.8793911] [0.88506088] [0.09528033]\n"
     ]
    }
   ],
   "source": [
    "print(\"Final hidden weights: \",end='')\n",
    "print(*W1)\n",
    "print(\"Final hidden bias: \",end='')\n",
    "print(*b1)\n",
    "print(\"Final output weights: \",end='')\n",
    "print(*W2)\n",
    "print(\"Final output bias: \",end='')\n",
    "print(*b2)\n",
    "\n",
    "print(\"\\nOutput from neural network after 10,000 epochs: \",end='')\n",
    "print(*Y_hat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alternatives & Improvements\n",
    "\n",
    "### Loss functions\n",
    "* Has significance on learning/updating\n",
    "* Choose according to the problem\n",
    "    * Regression: MSE, MSLE, MAE\n",
    "    * Binary Classification: Binary Cross-Entropy, Hinge loss,...\n",
    "    * Multi-class Classification: KL-divergence, Multi Cross-Entropy,...\n",
    "    \n",
    "### Activation functions\n",
    "* Linear function\n",
    "    * combinations are linear, not good approximative properties\n",
    "    * unbounded range\n",
    "    * constant learning rate\n",
    "* Sigmoids\n",
    "    * in [0,1]\n",
    "    * non-linear, good for stacking layers\n",
    "    * sensitive around zero\n",
    "    * \"vanishing gradients\" -- near zero learning rate\n",
    "* Tanh\n",
    "    * \"Scaled Sigmoid\", $2*sigmoid(2x)-1$\n",
    "    * steeper gradient\n",
    "* ReLu\n",
    "    * $A(x) = max(x,0)$\n",
    "    * does not fire up all activations as sigmoids -- that's costly in big networks\n",
    "    * combinations are non-linear -- good approximator\n",
    "    * less computationally demanding\n",
    "    * \"dying gradient\" -- neurons will stop responding because the gradient = 0 for negative values\n",
    "    * \"leaky ReLu\" -- $y=0.01x,\\, \\mathrm{for}\\, x<0$, the idea is to gradient recover itself\n",
    "    \n",
    "* Generally, ReLu are the most used one, but sigmoids can be better approximators (if more costly ones)\n",
    "\n",
    "### Learning step\n",
    "* some alternatives to backprop, but mostly minor\n",
    "* https://www.technologyreview.com/s/608911/is-ai-riding-a-one-trick-pony/\n",
    "* many alternatives to gradient descent!\n",
    "    * proximal grads\n",
    "    * evolutionary algorithms\n",
    "    * Ada optimizers\n",
    "    * stochastic gradient descent\n",
    "    \n",
    "    \n",
    "### Architecture\n",
    "* many variants of mostly deep networks\n",
    "* CNNs (use convolution instead of matrix multiplication) in one or more places\n",
    "* RNNs (use recurrence, long short-term memory -- good for time-series data, simulate lags)\n",
    "* more engineering than math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keras\n",
    "High-level framework for NNets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load our dataset\n",
    "data = 'data/attrition.csv'\n",
    "df = pd.read_csv(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's drop EmployeeID\n",
    "df.drop(['EmployeeNumber', 'EmployeeCount'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Age</th>\n",
       "      <th>Attrition</th>\n",
       "      <th>BusinessTravel</th>\n",
       "      <th>DailyRate</th>\n",
       "      <th>Department</th>\n",
       "      <th>DistanceFromHome</th>\n",
       "      <th>Education</th>\n",
       "      <th>EducationField</th>\n",
       "      <th>EnvironmentSatisfaction</th>\n",
       "      <th>Gender</th>\n",
       "      <th>HourlyRate</th>\n",
       "      <th>JobInvolvement</th>\n",
       "      <th>JobLevel</th>\n",
       "      <th>JobRole</th>\n",
       "      <th>JobSatisfaction</th>\n",
       "      <th>MaritalStatus</th>\n",
       "      <th>MonthlyIncome</th>\n",
       "      <th>MonthlyRate</th>\n",
       "      <th>NumCompaniesWorked</th>\n",
       "      <th>Over18</th>\n",
       "      <th>OverTime</th>\n",
       "      <th>PercentSalaryHike</th>\n",
       "      <th>PerformanceRating</th>\n",
       "      <th>RelationshipSatisfaction</th>\n",
       "      <th>StandardHours</th>\n",
       "      <th>StockOptionLevel</th>\n",
       "      <th>TotalWorkingYears</th>\n",
       "      <th>TrainingTimesLastYear</th>\n",
       "      <th>WorkLifeBalance</th>\n",
       "      <th>YearsAtCompany</th>\n",
       "      <th>YearsInCurrentRole</th>\n",
       "      <th>YearsSinceLastPromotion</th>\n",
       "      <th>YearsWithCurrManager</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>41</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Travel_Rarely</td>\n",
       "      <td>1102</td>\n",
       "      <td>Sales</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>Life Sciences</td>\n",
       "      <td>2</td>\n",
       "      <td>Female</td>\n",
       "      <td>94</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>Sales Executive</td>\n",
       "      <td>4</td>\n",
       "      <td>Single</td>\n",
       "      <td>5993</td>\n",
       "      <td>19479</td>\n",
       "      <td>8</td>\n",
       "      <td>Y</td>\n",
       "      <td>Yes</td>\n",
       "      <td>11</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>80</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>49</td>\n",
       "      <td>No</td>\n",
       "      <td>Travel_Frequently</td>\n",
       "      <td>279</td>\n",
       "      <td>Research &amp; Development</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>Life Sciences</td>\n",
       "      <td>3</td>\n",
       "      <td>Male</td>\n",
       "      <td>61</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>Research Scientist</td>\n",
       "      <td>2</td>\n",
       "      <td>Married</td>\n",
       "      <td>5130</td>\n",
       "      <td>24907</td>\n",
       "      <td>1</td>\n",
       "      <td>Y</td>\n",
       "      <td>No</td>\n",
       "      <td>23</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>80</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>37</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Travel_Rarely</td>\n",
       "      <td>1373</td>\n",
       "      <td>Research &amp; Development</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>Other</td>\n",
       "      <td>4</td>\n",
       "      <td>Male</td>\n",
       "      <td>92</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>Laboratory Technician</td>\n",
       "      <td>3</td>\n",
       "      <td>Single</td>\n",
       "      <td>2090</td>\n",
       "      <td>2396</td>\n",
       "      <td>6</td>\n",
       "      <td>Y</td>\n",
       "      <td>Yes</td>\n",
       "      <td>15</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>80</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>33</td>\n",
       "      <td>No</td>\n",
       "      <td>Travel_Frequently</td>\n",
       "      <td>1392</td>\n",
       "      <td>Research &amp; Development</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>Life Sciences</td>\n",
       "      <td>4</td>\n",
       "      <td>Female</td>\n",
       "      <td>56</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>Research Scientist</td>\n",
       "      <td>3</td>\n",
       "      <td>Married</td>\n",
       "      <td>2909</td>\n",
       "      <td>23159</td>\n",
       "      <td>1</td>\n",
       "      <td>Y</td>\n",
       "      <td>Yes</td>\n",
       "      <td>11</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>80</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>27</td>\n",
       "      <td>No</td>\n",
       "      <td>Travel_Rarely</td>\n",
       "      <td>591</td>\n",
       "      <td>Research &amp; Development</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>Medical</td>\n",
       "      <td>1</td>\n",
       "      <td>Male</td>\n",
       "      <td>40</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>Laboratory Technician</td>\n",
       "      <td>2</td>\n",
       "      <td>Married</td>\n",
       "      <td>3468</td>\n",
       "      <td>16632</td>\n",
       "      <td>9</td>\n",
       "      <td>Y</td>\n",
       "      <td>No</td>\n",
       "      <td>12</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>80</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Age Attrition  ... YearsSinceLastPromotion  YearsWithCurrManager\n",
       "0   41       Yes  ...                       0                     5\n",
       "1   49        No  ...                       1                     7\n",
       "2   37       Yes  ...                       0                     0\n",
       "3   33        No  ...                       3                     0\n",
       "4   27        No  ...                       2                     2\n",
       "\n",
       "[5 rows x 33 columns]"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split to input and predicted data\n",
    "Xk = df.loc[:,df.columns!='Attrition']\n",
    "Yk = df.loc[:,'Attrition']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we're going to reuse some tricks from the last lesson\n",
    "from sklearn.preprocessing import OneHotEncoder, RobustScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Age', 'Attrition', 'BusinessTravel', 'DailyRate', 'Department',\n",
       "       'DistanceFromHome', 'Education', 'EducationField',\n",
       "       'EnvironmentSatisfaction', 'Gender', 'HourlyRate', 'JobInvolvement',\n",
       "       'JobLevel', 'JobRole', 'JobSatisfaction', 'MaritalStatus',\n",
       "       'MonthlyIncome', 'MonthlyRate', 'NumCompaniesWorked', 'Over18',\n",
       "       'OverTime', 'PercentSalaryHike', 'PerformanceRating',\n",
       "       'RelationshipSatisfaction', 'StandardHours', 'StockOptionLevel',\n",
       "       'TotalWorkingYears', 'TrainingTimesLastYear', 'WorkLifeBalance',\n",
       "       'YearsAtCompany', 'YearsInCurrentRole', 'YearsSinceLastPromotion',\n",
       "       'YearsWithCurrManager'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's label numerical columns\n",
    "num_feature_cols = [ 'Age', 'DailyRate','DistanceFromHome', 'Education',\n",
    "        'HourlyRate', 'EnvironmentSatisfaction', 'JobInvolvement', 'JobLevel',\n",
    "        'JobSatisfaction', 'MonthlyIncome','NumCompaniesWorked', 'PercentSalaryHike',\n",
    "        'RelationshipSatisfaction', 'StockOptionLevel', 'PerformanceRating', 'StandardHours',\n",
    "        'TotalWorkingYears', 'TrainingTimesLastYear', 'WorkLifeBalance',\n",
    "        'YearsAtCompany', 'YearsInCurrentRole', 'YearsSinceLastPromotion',\n",
    "        'YearsWithCurrManager', 'MonthlyRate']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "# and categorical\n",
    "cat_feature_cols = [x for x in Xk.columns if x not in num_feature_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['BusinessTravel',\n",
       " 'Department',\n",
       " 'EducationField',\n",
       " 'Gender',\n",
       " 'JobRole',\n",
       " 'MaritalStatus',\n",
       " 'Over18',\n",
       " 'OverTime']"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cat_feature_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipelines!\n",
    "num_transformer = Pipeline(steps=[\n",
    "                  ('imputer', SimpleImputer(strategy='median')),\n",
    "                  ('scaler', RobustScaler())])\n",
    "\n",
    "cat_transformer = Pipeline(steps=[\n",
    "                  ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "                  ('onehot', OneHotEncoder(categories='auto', \n",
    "                                     sparse=False, \n",
    "                                     handle_unknown='ignore'))])\n",
    "\n",
    "pipeline_preprocess = ColumnTransformer(transformers=[\n",
    "        ('numerical_preprocessing', num_transformer, num_feature_cols),\n",
    "        ('categorical_preprocessing', cat_transformer, cat_feature_cols)],\n",
    "        remainder='passthrough')\n",
    "\n",
    "pipe0 = Pipeline([(\"transform_inputs\", pipeline_preprocess)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply transformations\n",
    "Xkk = pipe0.fit_transform(Xk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "# alternative way to get booleans\n",
    "Ykk = Yk.str.get_dummies().iloc[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xk_train, Xk_test, Yk_train, Yk_test = train_test_split(Xkk, Ykk, test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "#First Hidden Layer\n",
    "model.add(Dense(4, activation='relu', kernel_initializer='random_normal', input_dim=53))\n",
    "#Second  Hidden Layer\n",
    "model.add(Dense(4, activation='relu', kernel_initializer='random_normal'))\n",
    "#Output Layer\n",
    "model.add(Dense(1, activation='sigmoid', kernel_initializer='random_normal'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compiling the neural network\n",
    "model.compile(optimizer ='adam',loss='binary_crossentropy', metrics =['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1029/1029 [==============================] - 0s 258us/step - loss: 0.6671 - accuracy: 0.8387\n",
      "Epoch 2/100\n",
      "1029/1029 [==============================] - 0s 90us/step - loss: 0.5152 - accuracy: 0.8416\n",
      "Epoch 3/100\n",
      "1029/1029 [==============================] - 0s 79us/step - loss: 0.4092 - accuracy: 0.8416\n",
      "Epoch 4/100\n",
      "1029/1029 [==============================] - 0s 83us/step - loss: 0.3940 - accuracy: 0.8416\n",
      "Epoch 5/100\n",
      "1029/1029 [==============================] - 0s 94us/step - loss: 0.3806 - accuracy: 0.8416\n",
      "Epoch 6/100\n",
      "1029/1029 [==============================] - 0s 83us/step - loss: 0.3688 - accuracy: 0.8416\n",
      "Epoch 7/100\n",
      "1029/1029 [==============================] - 0s 94us/step - loss: 0.3580 - accuracy: 0.8416\n",
      "Epoch 8/100\n",
      "1029/1029 [==============================] - 0s 82us/step - loss: 0.3479 - accuracy: 0.8416\n",
      "Epoch 9/100\n",
      "1029/1029 [==============================] - 0s 85us/step - loss: 0.3396 - accuracy: 0.8416\n",
      "Epoch 10/100\n",
      "1029/1029 [==============================] - 0s 82us/step - loss: 0.3321 - accuracy: 0.8416\n",
      "Epoch 11/100\n",
      "1029/1029 [==============================] - 0s 85us/step - loss: 0.3256 - accuracy: 0.8416\n",
      "Epoch 12/100\n",
      "1029/1029 [==============================] - 0s 84us/step - loss: 0.3212 - accuracy: 0.8416\n",
      "Epoch 13/100\n",
      "1029/1029 [==============================] - 0s 85us/step - loss: 0.3172 - accuracy: 0.8416\n",
      "Epoch 14/100\n",
      "1029/1029 [==============================] - 0s 82us/step - loss: 0.3141 - accuracy: 0.8416\n",
      "Epoch 15/100\n",
      "1029/1029 [==============================] - 0s 91us/step - loss: 0.3108 - accuracy: 0.8416\n",
      "Epoch 16/100\n",
      "1029/1029 [==============================] - 0s 91us/step - loss: 0.3088 - accuracy: 0.8416\n",
      "Epoch 17/100\n",
      "1029/1029 [==============================] - 0s 87us/step - loss: 0.3070 - accuracy: 0.8659\n",
      "Epoch 18/100\n",
      "1029/1029 [==============================] - 0s 83us/step - loss: 0.3051 - accuracy: 0.8873\n",
      "Epoch 19/100\n",
      "1029/1029 [==============================] - 0s 86us/step - loss: 0.3045 - accuracy: 0.8834\n",
      "Epoch 20/100\n",
      "1029/1029 [==============================] - 0s 86us/step - loss: 0.3030 - accuracy: 0.8882\n",
      "Epoch 21/100\n",
      "1029/1029 [==============================] - 0s 84us/step - loss: 0.3016 - accuracy: 0.8912\n",
      "Epoch 22/100\n",
      "1029/1029 [==============================] - 0s 92us/step - loss: 0.3013 - accuracy: 0.8931\n",
      "Epoch 23/100\n",
      "1029/1029 [==============================] - 0s 83us/step - loss: 0.3009 - accuracy: 0.8892\n",
      "Epoch 24/100\n",
      "1029/1029 [==============================] - 0s 82us/step - loss: 0.2988 - accuracy: 0.8921\n",
      "Epoch 25/100\n",
      "1029/1029 [==============================] - 0s 87us/step - loss: 0.2977 - accuracy: 0.8912\n",
      "Epoch 26/100\n",
      "1029/1029 [==============================] - 0s 82us/step - loss: 0.2972 - accuracy: 0.8902\n",
      "Epoch 27/100\n",
      "1029/1029 [==============================] - 0s 83us/step - loss: 0.2969 - accuracy: 0.8921\n",
      "Epoch 28/100\n",
      "1029/1029 [==============================] - 0s 90us/step - loss: 0.2961 - accuracy: 0.8960\n",
      "Epoch 29/100\n",
      "1029/1029 [==============================] - 0s 89us/step - loss: 0.2951 - accuracy: 0.8902\n",
      "Epoch 30/100\n",
      "1029/1029 [==============================] - 0s 90us/step - loss: 0.2946 - accuracy: 0.8902\n",
      "Epoch 31/100\n",
      "1029/1029 [==============================] - 0s 88us/step - loss: 0.2940 - accuracy: 0.8921\n",
      "Epoch 32/100\n",
      "1029/1029 [==============================] - 0s 86us/step - loss: 0.2926 - accuracy: 0.8931\n",
      "Epoch 33/100\n",
      "1029/1029 [==============================] - 0s 84us/step - loss: 0.2931 - accuracy: 0.8950\n",
      "Epoch 34/100\n",
      "1029/1029 [==============================] - 0s 85us/step - loss: 0.2920 - accuracy: 0.8931\n",
      "Epoch 35/100\n",
      "1029/1029 [==============================] - 0s 85us/step - loss: 0.2911 - accuracy: 0.8902\n",
      "Epoch 36/100\n",
      "1029/1029 [==============================] - 0s 84us/step - loss: 0.2898 - accuracy: 0.8941\n",
      "Epoch 37/100\n",
      "1029/1029 [==============================] - 0s 85us/step - loss: 0.2885 - accuracy: 0.8931\n",
      "Epoch 38/100\n",
      "1029/1029 [==============================] - 0s 89us/step - loss: 0.2884 - accuracy: 0.8931\n",
      "Epoch 39/100\n",
      "1029/1029 [==============================] - 0s 90us/step - loss: 0.2880 - accuracy: 0.8931\n",
      "Epoch 40/100\n",
      "1029/1029 [==============================] - 0s 96us/step - loss: 0.2875 - accuracy: 0.8912\n",
      "Epoch 41/100\n",
      "1029/1029 [==============================] - 0s 93us/step - loss: 0.2862 - accuracy: 0.8970\n",
      "Epoch 42/100\n",
      "1029/1029 [==============================] - 0s 92us/step - loss: 0.2851 - accuracy: 0.8941\n",
      "Epoch 43/100\n",
      "1029/1029 [==============================] - 0s 85us/step - loss: 0.2844 - accuracy: 0.8960\n",
      "Epoch 44/100\n",
      "1029/1029 [==============================] - 0s 86us/step - loss: 0.2837 - accuracy: 0.8960\n",
      "Epoch 45/100\n",
      "1029/1029 [==============================] - 0s 86us/step - loss: 0.2824 - accuracy: 0.8960\n",
      "Epoch 46/100\n",
      "1029/1029 [==============================] - 0s 101us/step - loss: 0.2829 - accuracy: 0.8970\n",
      "Epoch 47/100\n",
      "1029/1029 [==============================] - 0s 100us/step - loss: 0.2827 - accuracy: 0.8989\n",
      "Epoch 48/100\n",
      "1029/1029 [==============================] - 0s 96us/step - loss: 0.2815 - accuracy: 0.8970\n",
      "Epoch 49/100\n",
      "1029/1029 [==============================] - 0s 88us/step - loss: 0.2806 - accuracy: 0.9009\n",
      "Epoch 50/100\n",
      "1029/1029 [==============================] - 0s 88us/step - loss: 0.2793 - accuracy: 0.8999\n",
      "Epoch 51/100\n",
      "1029/1029 [==============================] - 0s 86us/step - loss: 0.2793 - accuracy: 0.8941\n",
      "Epoch 52/100\n",
      "1029/1029 [==============================] - 0s 87us/step - loss: 0.2788 - accuracy: 0.8970\n",
      "Epoch 53/100\n",
      "1029/1029 [==============================] - 0s 95us/step - loss: 0.2788 - accuracy: 0.9009\n",
      "Epoch 54/100\n",
      "1029/1029 [==============================] - 0s 88us/step - loss: 0.2772 - accuracy: 0.9018\n",
      "Epoch 55/100\n",
      "1029/1029 [==============================] - 0s 85us/step - loss: 0.2772 - accuracy: 0.8989\n",
      "Epoch 56/100\n",
      "1029/1029 [==============================] - 0s 88us/step - loss: 0.2766 - accuracy: 0.8999\n",
      "Epoch 57/100\n",
      "1029/1029 [==============================] - 0s 82us/step - loss: 0.2760 - accuracy: 0.8989\n",
      "Epoch 58/100\n",
      "1029/1029 [==============================] - 0s 84us/step - loss: 0.2752 - accuracy: 0.9028\n",
      "Epoch 59/100\n",
      "1029/1029 [==============================] - 0s 84us/step - loss: 0.2752 - accuracy: 0.8989\n",
      "Epoch 60/100\n",
      "1029/1029 [==============================] - 0s 97us/step - loss: 0.2751 - accuracy: 0.8989\n",
      "Epoch 61/100\n",
      "1029/1029 [==============================] - 0s 86us/step - loss: 0.2737 - accuracy: 0.9009\n",
      "Epoch 62/100\n",
      "1029/1029 [==============================] - 0s 85us/step - loss: 0.2730 - accuracy: 0.9009\n",
      "Epoch 63/100\n",
      "1029/1029 [==============================] - 0s 88us/step - loss: 0.2730 - accuracy: 0.9018\n",
      "Epoch 64/100\n",
      "1029/1029 [==============================] - 0s 97us/step - loss: 0.2722 - accuracy: 0.8980\n",
      "Epoch 65/100\n",
      "1029/1029 [==============================] - 0s 94us/step - loss: 0.2716 - accuracy: 0.8999\n",
      "Epoch 66/100\n",
      "1029/1029 [==============================] - 0s 97us/step - loss: 0.2707 - accuracy: 0.9009\n",
      "Epoch 67/100\n",
      "1029/1029 [==============================] - 0s 89us/step - loss: 0.2706 - accuracy: 0.9009\n",
      "Epoch 68/100\n",
      "1029/1029 [==============================] - 0s 80us/step - loss: 0.2707 - accuracy: 0.9009\n",
      "Epoch 69/100\n",
      "1029/1029 [==============================] - 0s 82us/step - loss: 0.2703 - accuracy: 0.9038\n",
      "Epoch 70/100\n",
      "1029/1029 [==============================] - 0s 84us/step - loss: 0.2698 - accuracy: 0.9038\n",
      "Epoch 71/100\n",
      "1029/1029 [==============================] - 0s 82us/step - loss: 0.2686 - accuracy: 0.8989\n",
      "Epoch 72/100\n",
      "1029/1029 [==============================] - 0s 93us/step - loss: 0.2686 - accuracy: 0.9048\n",
      "Epoch 73/100\n",
      "1029/1029 [==============================] - 0s 114us/step - loss: 0.2683 - accuracy: 0.9057\n",
      "Epoch 74/100\n",
      "1029/1029 [==============================] - 0s 96us/step - loss: 0.2677 - accuracy: 0.9038\n",
      "Epoch 75/100\n",
      "1029/1029 [==============================] - 0s 92us/step - loss: 0.2677 - accuracy: 0.9057\n",
      "Epoch 76/100\n",
      "1029/1029 [==============================] - 0s 90us/step - loss: 0.2666 - accuracy: 0.8999\n",
      "Epoch 77/100\n",
      "1029/1029 [==============================] - 0s 86us/step - loss: 0.2659 - accuracy: 0.9038\n",
      "Epoch 78/100\n",
      "1029/1029 [==============================] - 0s 90us/step - loss: 0.2663 - accuracy: 0.9018\n",
      "Epoch 79/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1029/1029 [==============================] - 0s 89us/step - loss: 0.2661 - accuracy: 0.9077\n",
      "Epoch 80/100\n",
      "1029/1029 [==============================] - 0s 84us/step - loss: 0.2649 - accuracy: 0.8999\n",
      "Epoch 81/100\n",
      "1029/1029 [==============================] - 0s 88us/step - loss: 0.2649 - accuracy: 0.9057\n",
      "Epoch 82/100\n",
      "1029/1029 [==============================] - 0s 90us/step - loss: 0.2648 - accuracy: 0.9067\n",
      "Epoch 83/100\n",
      "1029/1029 [==============================] - 0s 89us/step - loss: 0.2641 - accuracy: 0.9048\n",
      "Epoch 84/100\n",
      "1029/1029 [==============================] - 0s 90us/step - loss: 0.2634 - accuracy: 0.9067\n",
      "Epoch 85/100\n",
      "1029/1029 [==============================] - 0s 86us/step - loss: 0.2626 - accuracy: 0.9086\n",
      "Epoch 86/100\n",
      "1029/1029 [==============================] - 0s 87us/step - loss: 0.2630 - accuracy: 0.9018\n",
      "Epoch 87/100\n",
      "1029/1029 [==============================] - 0s 86us/step - loss: 0.2622 - accuracy: 0.9096\n",
      "Epoch 88/100\n",
      "1029/1029 [==============================] - 0s 89us/step - loss: 0.2616 - accuracy: 0.9106\n",
      "Epoch 89/100\n",
      "1029/1029 [==============================] - 0s 85us/step - loss: 0.2624 - accuracy: 0.9077\n",
      "Epoch 90/100\n",
      "1029/1029 [==============================] - 0s 83us/step - loss: 0.2610 - accuracy: 0.9067\n",
      "Epoch 91/100\n",
      "1029/1029 [==============================] - 0s 85us/step - loss: 0.2618 - accuracy: 0.9048\n",
      "Epoch 92/100\n",
      "1029/1029 [==============================] - 0s 83us/step - loss: 0.2608 - accuracy: 0.9077\n",
      "Epoch 93/100\n",
      "1029/1029 [==============================] - 0s 82us/step - loss: 0.2600 - accuracy: 0.9067\n",
      "Epoch 94/100\n",
      "1029/1029 [==============================] - 0s 82us/step - loss: 0.2596 - accuracy: 0.9067\n",
      "Epoch 95/100\n",
      "1029/1029 [==============================] - 0s 87us/step - loss: 0.2603 - accuracy: 0.9067\n",
      "Epoch 96/100\n",
      "1029/1029 [==============================] - 0s 84us/step - loss: 0.2595 - accuracy: 0.9086\n",
      "Epoch 97/100\n",
      "1029/1029 [==============================] - 0s 85us/step - loss: 0.2593 - accuracy: 0.9067\n",
      "Epoch 98/100\n",
      "1029/1029 [==============================] - 0s 88us/step - loss: 0.2589 - accuracy: 0.9086\n",
      "Epoch 99/100\n",
      "1029/1029 [==============================] - 0s 85us/step - loss: 0.2582 - accuracy: 0.9077\n",
      "Epoch 100/100\n",
      "1029/1029 [==============================] - 0s 90us/step - loss: 0.2581 - accuracy: 0.9067\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x138f8b550>"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(Xk_train,Yk_train, batch_size=10, epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1029/1029 [==============================] - 0s 26us/step\n",
      "Loss and accuracy on train set# 0.255008: 0.908649\n"
     ]
    }
   ],
   "source": [
    "[loss_train, accuracy_train] = model.evaluate(Xk_train, Yk_train)\n",
    "print('Loss and accuracy on train set {:f}: {:f}'.format(loss_train, accuracy_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's predict on test set\n",
    "Yk_pred = model.predict(Xk_test)\n",
    "Yk_pred = (Yk_pred>0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix, without normalization\n",
      "[[355  12]\n",
      " [ 51  23]] \n",
      "\n",
      "Normalized confusion matrix\n",
      "[[0.96730245 0.03269755]\n",
      " [0.68918919 0.31081081]] \n",
      "\n",
      "Accuracy of classification 0.857143\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "cm = confusion_matrix(Yk_test, Yk_pred)\n",
    "accuracy = accuracy_score(Yk_test, Yk_pred)\n",
    "cm_norm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "print('Confusion matrix, without normalization')\n",
    "print(cm,'\\n')\n",
    "print('Normalized confusion matrix')\n",
    "print(cm_norm,'\\n')\n",
    "print('Accuracy of classification {:f}'.format(accuracy))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
